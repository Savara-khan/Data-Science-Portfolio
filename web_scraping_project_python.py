# -*- coding: utf-8 -*-
"""Web Scraping Project Python

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J_0TRH6U-IBX2K707phbMShfyK6cOAIl

#**Web Scraping Project: Data Discovery and Analysis **

**This web scraping project has tested my ability to navigate the digital world while gathering and evaluating data from any website. The objective is to collect at least 50 data points, validate, clean, and format the data using regular expressions, and show the data's significant insights using pandas and or other powered analysis. This study explores data handling, cleansing, and analysis practicallyâ€”essential skills in the world of data science.Prepare to discover insightful things!**
"""

# Question: How many of each top cuisine restaurants are in NYC?

#Topic: Location NYC, Best Match for me 50 data points

# Data Collection [30 points]
# Choose a website with diverse and ample data
# Explain why the chosen website is a good source in the comments

# Web Crawling [15 points]
# Use scrapy or another similar library for web crawling
# Ensure ethical scraping practices and handle dynamic content
# Implement a Python script to navigate through multiple pages

# Web Scraping [15 points]
# Use requests or selenium for getting responses from URLs
# Use BeautifulSoup or any preferred library for parsing HTML
# Scrape a minimum of 50 data points
# Extract relevant information from HTML

# Regular Expressions [15 points]
# Apply 3 different regular expressions using the python re library
# Extract specific patterns from the raw HTML data

# Data Cleaning [20 points]
# Use pandas to clean up the data
# Handle missing values, duplicates, and anomalies
# Choose appropriate strategies for dealing with incomplete/bad data
# Organize data into a well-formatted DataFrame

# Pandas Powered Analysis [20 points]
#Utilize pandas for in-depth analysis, data manipulation, and statistical insights.
#Create clear visualizations

"""Install and Import ALL Necessary Libraries"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install scrapy tabulate, bs4

import re
import requests
import pandas as pd
import matplotlib.pyplot as plt
import datetime
from collections import Counter, defaultdict
from tabulate import tabulate
from bs4 import BeautifulSoup

"""Web Crawling / Web Scraping"""

today = datetime.date.today()
api_key = '6fnGj5y8pz7JG15PVWJgZ7KMcegdW5wEkBORY3BGNZtHIax4p1D6cMm2b419q2ISiaURfJ5EeJsmoHdKqVZQV_d0tpUz6M6xswdo0AlJegcHIJp3GtH4IVPbTKNnZXYx'
url = 'https://api.yelp.com/v3/businesses/search'


# collect the data
print("[INFO] making API call")
response = requests.get(
    url,
    params={
    'location': 'nyc',
    'sort_by': 'best_match',
    'limit': 50
     },
     headers={
     "accept": "application/json",
     "Authorization": f"Bearer {api_key}"
     }
)

# Extract relevant data into a list of dictionaries
businesses_data = []
cuisines = defaultdict(int)
if response.status_code == 200:
    api_response = response.json()

    print(f"[INFO] parsing API response")
    for business in api_response.get("businesses", []):
        display_phone = business.get("display_phone")
        business_data = {
            "name": business.get("name"),
            "categories": ",".join([category["title"] for category in business.get("categories", [])]),
            "latitude": business["coordinates"]["latitude"],
            "longitude": business["coordinates"]["longitude"],
            "display_phone": business.get("display_phone", ""),
            "distance": business.get("distance", 0),
            "address": ", ".join(business["location"]["display_address"]),
            "rating": business.get("rating", 0),
            "review_count": business.get("review_count", 0),
            "url": business.get("url"),
            "phone": ("+" + re.sub(r'\D', '', display_phone)) if display_phone else None
        }
        businesses_data.append(business_data)
        for cusine in [category["title"] for category in business.get("categories", [])]:
            cuisines[cusine] +=1

    print(f"[INFO] collected {len(businesses_data)} items")
else:
    print(f'Failed to retrieve data. Status code: {response.status_code}')

dishes = []

print("[INFO] collecting dishes")
for i,restaurant in enumerate(businesses_data):
    print(f"\t\t [INFO] {i+1}/{len(businesses_data)}")
    try:
        name = restaurant["name"]
        url = restaurant["url"]

        # Get HTML content from the restaurant's menu page
        response = requests.get(url)
        soup = BeautifulSoup(response.content, 'html.parser')

        # Find all dish elements on the menu page
        dish_elements = soup.find_all('div', class_='dishWrapper__09f24__Bj2sT')

        for dish_element in dish_elements[:5]:
            try:
                # Extract dish details
                dish_name = dish_element.find('p', class_='css-nyjpex').text.strip()
                dish_url = dish_element.find('a')['href']
                dish_image_url = dish_element.find('img')['src']

                # Extract optional reviews using re.search
                reviews_match = re.search(r'(\d+).+Reviews', dish_element.text)
                dish_reviews = int(reviews_match.group(1)) if reviews_match else None

                # Create a dictionary for the dish
                dish_info = {
                    'restaurant_name': name,
                    'dish_name': dish_name,
                    'url': dish_url,
                    'image_url': dish_image_url,
                    'reviews': dish_reviews
                }

                # Append dish dictionary to the list
                dishes.append(dish_info)
            except Exception as e:
                print(f"Error processing dish: {e}")
    except Exception as e:
        print(f"Error processing restaurant: {e}")

print(f"[INFO] collected {len(dishes)} dishes")

import os
import requests
import pandas as pd
import matplotlib.pyplot as plt
from tabulate import tabulate
import datetime

# Define request params
today = datetime.date.today()
api_key = '6fnGj5y8pz7JG15PVWJgZ7KMcegdW5wEkBORY3BGNZtHIax4p1D6cMm2b419q2ISiaURfJ5EeJsmoHdKqVZQV_d0tpUz6M6xswdo0AlJegcHIJp3GtH4IVPbTKNnZXYx'
url = 'https://api.yelp.com/v3/businesses/search'

params = {
    'location': 'nyc',
    'sort_by': 'best_match',
    'limit': 50
}

headers = {
    "accept": "application/json",
    "Authorization": f"Bearer {api_key}"
}

# Define an empty DataFrame
df = pd.DataFrame()


response = requests.get(url, params=params, headers=headers)

if response.status_code == 200:
    # Parse JSON data
    data = response.json()
    businesses = data.get('businesses', [])

    # Extract relevant data into a DataFrame
    df = pd.DataFrame([
        {
            'Name': business.get('name', ''),
            'Location': business.get('location', {}).get('address1', ''),
            'Cuisine': business.get('categories', [{}])[0].get('title', ''),
            'Price': business.get('price', '')
        }
        for business in businesses
    ])

    # Display raw data
    print("Raw Data:")
    print(df)

    # Count occurrences of each cuisine
    cuisine_counts = df['Cuisine'].value_counts()

    # Display summary statistics
    print("\nSummary Statistics:")
    print(tabulate(df, headers='keys', tablefmt='pretty', showindex=False))

"""Clean the Collected Data

"""

businesses_df = pd.DataFrame(businesses_data)
businesses_df.head()

businesses_df.isnull().sum()

businesses_df_cleaned = businesses_df.dropna(axis=0)

cuisines_df = pd.DataFrame([{"Cuisine":k, "Occurrences":v} for k,v in cuisines.items()  ])
cuisines_df.head()

dishes_df = pd.DataFrame(dishes)
dishes_df.head()

dishes_df['dish_name'].unique()

"""Visualize the data"""

plt.figure(figsize=(12, 6))
plt.bar(cuisines_df['Cuisine'], cuisines_df['Occurrences'])
plt.title('Cuisine Distribution')
plt.xlabel('Cuisine')
plt.ylabel('Number of Occurrences')
plt.xticks(rotation=45, ha='right')
plt.show()

"""Save the collected data"""

import os

# Create a directory named "data" if it doesn't exist
data_directory = "data"
os.makedirs(data_directory, exist_ok=True)

# Save dishes_df to CSV
dishes_df.to_csv(os.path.join(data_directory, "dishes_data.csv"), index=False)

# Save businesses_df_cleaned to CSV
businesses_df_cleaned.to_csv(os.path.join(data_directory, "businesses_data.csv"), index=False)

# Save cuisines_df to CSV
cuisines_df.to_csv(os.path.join(data_directory, "cuisines_data.csv"), index=False)

"""**What question are you solving?**

- The provided Python script utilizes Yelp's API and web scraping techniques to collect comprehensive data from Yelp on how many of each top cuisine restaurants in New York City near my area. It initiates an API call to retrieve restaurant details, including names, categories, coordinates, ratings, and more. Simultaneously, the script scrapes restaurant menu pages to compile information on the top five popular dishes per restaurant. The collected data is cleaned, organized into pandas DataFrames, and visualized through a bar graph showcasing the distribution of different cuisines. The script concludes by saving the cleaned restaurant, dish, and cuisine data into separate CSV files within a "data" directory. Overall, this script serves as a versatile tool for obtaining and analyzing diverse information about restaurants and their offerings from Yelp.



**How did you go about collecting/cleaning your data?**

- The data collection and cleaning process involves an initial API call to Yelp for restaurant details, followed by scraping each restaurant's page for information on the top popular dishes. The obtained data is organized into pandas DataFrames for restaurants, cuisines, and dishes. Cleaning steps include handling missing values in the restaurant dataset, creating a structured cuisine data frame, and visualizing unique dishes. Matplotlib is used for a bar graph showcasing cuisine distribution. Finally, the cleaned data is saved into CSV files within a dedicated "data" directory, ensuring a well-organized and ready-to-use dataset for further analysis.



**What challenges did you face while collecting data?**

- While collecting data, one challenge was the need to make an API call to Yelp for restaurant details, requiring an API key for authentication. Additionally, web scraping to extract information on popular dishes from each restaurant's page presented challenges due to variations in HTML structure. Ensuring robust error handling and exception management was crucial to address potential disruptions in the scraping process, especially when dealing with diverse restaurant websites. These challenges highlight the importance of adaptability in handling different data sources and structures to ensure a comprehensive and accurate dataset.



**How did you choose to represent your data?**

- The data was represented using Pandas DataFrames in Python. Businesses' information, including names, categories, locations, ratings, and review counts, was structured into a DataFrame for easy manipulation and analysis. Similarly, the details of popular dishes, such as restaurant names, dish names, URLs, image URLs, and reviews, were organized into a separate DataFrame. This tabular representation facilitated efficient data cleaning, analysis, and visualization. Cuisines were summarized using a DataFrame that captured the occurrences of each cuisine. The tabular format allows for straightforward export to CSV files, supporting future accessibility and utilization in various analytical tools.



**If you had more time, what else would you add to your project?**

- If given more time, I would enhance the project in several ways. Firstly, I would explore further data visualization techniques, such as interactive dashboards, to provide users with a more engaging and comprehensive exploration of the collected data. I would also consider incorporating machine learning models to derive insights or predictions based on the available data. Lastly, I would work on optimizing the code for scalability, allowing it to handle larger datasets and accommodate additional features for a more robust and versatile tool.



**If you had more knowledge (ML, Stats, more coding), what would you like to add?**

- With more knowledge in machine learning, statistics, and advanced coding, I would integrate predictive modeling and data analytics into the project. This could involve employing machine learning algorithms to predict various outcomes, such as a restaurant's popularity based on certain features or forecasting trends in the restaurant industry. Statistical analysis could be applied to identify significant patterns or correlations in the data. Overall, a more comprehensive and insightful analysis could be achieved with advanced ML and statistical techniques, enhancing the project's capabilities.



"""